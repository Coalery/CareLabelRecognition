{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"label_recognition-v2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"16vfE7uzexJerXLVysHP22dykjkNr6TYT","authorship_tag":"ABX9TyOW4coSQi6NPhml0qnOeIA6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"po7THSajpC80"},"source":["## 세탁 라벨 분류 CNN 모델\n","세탁 라벨을 찍으면 그를 통해 라벨을 분류하는 CNN 모델입니다.\n","\n","https://debuggercafe.com/multi-label-image-classification-with-pytorch-and-deep-learning/"]},{"cell_type":"code","metadata":{"id":"yk41acS8-0Kg","executionInfo":{"status":"ok","timestamp":1637643419687,"user_tz":-540,"elapsed":2037,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01821074225687842365"}}},"source":["import torch\n","import cv2\n","import matplotlib\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset\n","from torchvision import models as models\n","import torch.nn as nn\n","from torch.utils.data import DataLoader"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"1PyI-R3S0i4h","executionInfo":{"status":"ok","timestamp":1637643419689,"user_tz":-540,"elapsed":18,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01821074225687842365"}}},"source":["class ImageDataset(Dataset):\n","    def __init__(self, csv, train, test, path='/content/drive/MyDrive/train/v2/train'):\n","        self.csv = csv\n","        self.train = train\n","        self.test = test\n","        self.path = path\n","\n","        self.all_image_names = self.csv[:]['id']\n","        self.all_labels = np.array(self.csv.drop(['id', 'type'], axis=1))\n","        self.train_ratio = int(0.85 * len(self.csv))\n","        self.valid_ratio = len(self.csv) - self.train_ratio\n","\n","        if self.train == True:\n","            print(f\"Number of training images: {self.train_ratio}\")\n","            self.image_names = list(self.all_image_names[:self.train_ratio])\n","            self.labels = list(self.all_labels[:self.train_ratio])\n","            self.transform = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.RandomHorizontalFlip(p=0.5),\n","                transforms.RandomRotation(degrees=45),\n","                transforms.ToTensor(),\n","            ])\n","        elif self.train == False and self.test == False:\n","            print(f\"Number of validation images: {self.valid_ratio}\")\n","            self.image_names = list(self.all_image_names[-self.valid_ratio:-10])\n","            self.labels = list(self.all_labels[-self.valid_ratio:])\n","            self.transform = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.ToTensor(),\n","            ])\n","        elif self.test == True and self.train == False:\n","            self.image_names = list(self.all_image_names[-10:])\n","            self.labels = list(self.all_labels[-10:])\n","            self.transform = transforms.Compose([\n","                transforms.ToPILImage(),\n","                transforms.ToTensor(),\n","            ])\n","    def __len__(self):\n","        return len(self.image_names)\n","    \n","    def __getitem__(self, index):\n","        image = cv2.imread(f\"{self.path}/{self.image_names[index]}\")\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = self.transform(image)\n","        targets = self.labels[index]\n","        \n","        return {\n","            'image': torch.tensor(image, dtype=torch.float32),\n","            'label': torch.tensor(targets, dtype=torch.float32)\n","        }"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KT7STzBT1g38","executionInfo":{"status":"ok","timestamp":1637643419690,"user_tz":-540,"elapsed":18,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01821074225687842365"}}},"source":["def model_f(pretrained, requires_grad):\n","    model = models.resnet50(progress=True, pretrained=pretrained)\n","    if requires_grad == False:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    elif requires_grad == True:\n","        for param in model.parameters():\n","            param.requires_grad = True\n","    model.fc = nn.Linear(2048, 6)\n","    return model"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"_PJcnKfo1tJJ","executionInfo":{"status":"ok","timestamp":1637643419691,"user_tz":-540,"elapsed":17,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01821074225687842365"}}},"source":["def train(model, dataloader, optimizer, criterion, train_data, device):\n","    print('Training')\n","    model.train()\n","    counter = 0\n","    train_running_loss = 0.0\n","    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n","        counter += 1\n","        data, target = data['image'].to(device), data['label'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(data)\n","        outputs = torch.sigmoid(outputs)\n","        loss = criterion(outputs, target)\n","        train_running_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    train_loss = train_running_loss / counter\n","    return train_loss"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcUvP0Ol1wc0","executionInfo":{"status":"ok","timestamp":1637643419692,"user_tz":-540,"elapsed":17,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01821074225687842365"}}},"source":["def validate(model, dataloader, criterion, val_data, device):\n","    print('Validating')\n","    model.eval()\n","    counter = 0\n","    val_running_loss = 0.0\n","    with torch.no_grad():\n","        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n","            counter += 1\n","            data, target = data['image'].to(device), data['label'].to(device)\n","            outputs = model(data)\n","            outputs = torch.sigmoid(outputs)\n","            loss = criterion(outputs, target)\n","            val_running_loss += loss.item()\n","        \n","        val_loss = val_running_loss / counter\n","        return val_loss"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"LI-bMKwM1zVH","executionInfo":{"status":"ok","timestamp":1637643420268,"user_tz":-540,"elapsed":592,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01821074225687842365"}}},"source":["matplotlib.style.use('ggplot')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = model_f(pretrained=True, requires_grad=False).to(device)\n","\n","lr = 0.0001\n","epochs = 20\n","batch_size = 32\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","criterion = nn.BCELoss()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DGtGkivA19Fd","executionInfo":{"status":"ok","timestamp":1637643420269,"user_tz":-540,"elapsed":7,"user":{"displayName":"김현우","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01821074225687842365"}},"outputId":"385719ad-7636-4638-c6de-bb39aacea91b"},"source":["train_csv = pd.read_csv('/content/drive/MyDrive/train/v2/data.csv')\n","train_data = ImageDataset(\n","    train_csv, train=True, test=False\n",")\n","# validation dataset\n","valid_data = ImageDataset(\n","    train_csv, train=False, test=False\n",")\n","# train data loader\n","train_loader = DataLoader(\n","    train_data, \n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","# validation data loader\n","valid_loader = DataLoader(\n","    valid_data, \n","    batch_size=batch_size,\n","    shuffle=False\n",")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training images: 5525\n","Number of validation images: 975\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TuUix0p628os","outputId":"b3c48988-1604-4d06-a0b6-d3aa66a2072c"},"source":["train_loss = []\n","valid_loss = []\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch+1} of {epochs}\")\n","    train_epoch_loss = train(\n","        model, train_loader, optimizer, criterion, train_data, device\n","    )\n","    valid_epoch_loss = validate(\n","        model, valid_loader, criterion, valid_data, device\n","    )\n","    train_loss.append(train_epoch_loss)\n","    valid_loss.append(valid_epoch_loss)\n","    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n","    print(f'Val Loss: {valid_epoch_loss:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 of 20\n","Training\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/172 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"," 66%|██████▋   | 114/172 [12:46<06:14,  6.45s/it]"]}]},{"cell_type":"code","metadata":{"id":"B1lQE2Bn3BLq"},"source":["# model save\n","# torch.save({\n","#             'epoch': epochs,\n","#             'model_state_dict': model.state_dict(),\n","#             'optimizer_state_dict': optimizer.state_dict(),\n","#             'loss': criterion,\n","#             }, '../outputs/model.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6e1NmZE3Cyl"},"source":["plt.figure(figsize=(10, 7))\n","plt.plot(train_loss, color='orange', label='train loss')\n","plt.plot(valid_loss, color='red', label='validataion loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2cgwuT33cOx"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"31G7gafoDm1J"},"source":["test_csv = pd.read_csv('/content/drive/MyDrive/train/v2/test.csv')\n","types = test_csv.columns.values[2:]\n","\n","test_data = ImageDataset(test_csv, train=False, test=True, path='/content/drive/MyDrive/train/v2/test')\n","test_loader = DataLoader(test_data, batch_size = 1, shuffle=False)\n","\n","def compare(item1):\n","  return item1[1]\n","\n","for counter, data in enumerate(test_loader):\n","    image, target = data['image'].to(device), data['label']\n","    target_indices = [i for i in range(len(target[0])) if target[0][i] == 1]\n","\n","    outputs = model(image)\n","    outputs = torch.sigmoid(outputs)\n","    outputs = outputs.detach().cpu()\n","\n","    actuals = [types[v] for v in target_indices]\n","    predict_result = []\n","    \n","    print('Predict:')\n","    for i in range(len(outputs[0])):\n","        predict_result.append((types[i], outputs[0][i]))\n","    predict_result = sorted(predict_result, key=compare, reverse=True)\n","    \n","    for item in predict_result:\n","      print('{0} ({1:0.03f}%)'.format(item[0], item[1] * 100), end='')\n","      if item[0] in actuals:\n","        print(' O', end='')\n","      print('')"],"execution_count":null,"outputs":[]}]}